# Metric-based evaluation: Requires Ground-truth labels

# I. End-to-end Metrics
## Natural Language Understanding (NLU)
### 1. Exact Match (EM)
### 2. F1
### 3. Precision
### 4. Recall
<br>

## Natural Language Generation (NLG)
### 1. BLEU
### 2. ROUGE

<br><br>

# II. Retrieval Metrics
## 1. Hit Rate (HR)

Measure the number of queries in which the correct contexts are included inside the retrieved contexts

$$ Hit rate = {{Queries containing correct contexts} \over {Total number of queries}} $$

## 2. Mean Average Precision (MAP)


## 3. Mean Reciprocal Rank (MRR)


## 4. Normalized Discounted Cumulative Gain (NDCR)