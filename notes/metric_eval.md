# Metric-based evaluation: Requires Ground-truth labels

References:

1. [Zuzanna Deutschman - Recommender Systems: Machine Learning Metrics and Business Metrics](https://neptune.ai/blog/recommender-systems-metrics#:~:text=Average%20precision%20(AP)&text=Precision%405%20equals%20%E2%85%95%20because,top%20ranking%20the%20correct%20recommendations)
2. [Benjamin Wang - Ranking Evaluation Metrics for Recommender Systems](https://towardsdatascience.com/ranking-evaluation-metrics-for-recommender-systems-263d0a66ef54)

<br>

# I. End-to-end Metrics
## 1. Natural Language Understanding (NLU)
### Classification:
- Accuracy
- Precision
- Recall
- F1
### Regression:
- MSE
- RMSE
- Degree of Agreement
<br>

## Natural Language Generation (NLG)
### 1. Exact Match (EM)

### 1. BLEU

### 2. ROUGE

### 3. METEOR
<br><br>
